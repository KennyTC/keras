{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Sequential model and Dense layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. simplest flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "# Add an input layer and a hidden layer with 10 neurons\n",
    "model.add(Dense(10, input_shape=(2,), activation=\"relu\"))\n",
    "# Add a 1-neuron output layer\n",
    "model.add(Dense(1))\n",
    "# Summarise your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "print(\"Training started..., this can take a while:\")\n",
    "# Fit your model on your data for 30 epochs\n",
    "model.fit(X_train, y_train, epochs = 30)\n",
    "# Evaluate your model \n",
    "print(\"Final lost value:\",model.evaluate(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use sigmoid as output, and loss = \"binary_crossentropy\", and output is 1 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "# Add a dense layer \n",
    "model.add(Dense(1, input_shape=(4,), activation=\"sigmoid\"))\n",
    "# Compile your model\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])\n",
    "# Display a summary of your model\n",
    "model.summary()\n",
    "# Train your model for 20 epochs\n",
    "model.fit(X_train,y_train, epochs=20)\n",
    "# Evaluate your model accuracy on the test set\n",
    "accuracy = model.evaluate(X_test,y_test)[1]\n",
    "# Print accuracy\n",
    "print('Accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use softmax, and loss = \"categorical_crossentropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a sequential model\n",
    "model = Sequential()\n",
    "  \n",
    "# Add 3 dense layers of 128, 64 and 32 neurons each\n",
    "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))  \n",
    "# Add a dense layer with as many neurons as competitors\n",
    "model.add(Dense(4, activation=\"softmax\"))  \n",
    "# Compile your model using categorical_crossentropy loss\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# Train your model on the training data for 200 epochs\n",
    "model.fit(X_train, y_train,epochs=200)\n",
    "# Evaluate your model accuracy on the test data\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new data\n",
    "preds = model.predict(X_new)\n",
    "# Print preds vs true values\n",
    "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{} | {}\".format(pred,X_new[i]))\n",
    "\n",
    "# Extract the indexes of the highest probable predictions\n",
    "preds = [np.argmax(pred) for pred in preds]\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:10} | {}\".format('Rounded Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{:25} | {}\".format(pred, X_new[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Multi label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we still use \"sigmoid\" for output,  loss is \"binary_crossentropy\" just like the case classification. But the units at output layer is equal to the number of classes we have. \n",
    "- E.g If our multi-label problems has 3 labels, the output layer should output 3 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer of 64 neurons and a 20 neuron's input\n",
    "model.add(Dense(64, input_shape=(20,), activation=\"relu\"))\n",
    "\n",
    "# Add an output layer of 3 neurons with sigmoid activation\n",
    "model.add(Dense(3, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile your model with adam and binary crossentropy loss\n",
    "model.compile(optimizer=\"adam\",\n",
    "           loss = \"binary_crossentropy\",\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The history callback is returned by default every time you train a model with the .fit() method. To access these metrics you can access the history dictionary inside the returned callback object and the corresponding keys.\n",
    "- Depends on the loss, metrics we define, history will tracks those things after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "# Add a hidden layer of 64 neurons and a 20 neuron's input\n",
    "model.add(Dense(64, input_shape=(20,), activation=\"relu\"))\n",
    "# Add an output layer of 3 neurons with sigmoid activation\n",
    "model.add(Dense(3, activation=\"sigmoid\"))\n",
    "# Compile your model with adam and binary crossentropy loss\n",
    "model.compile(optimizer=\"adam\",\n",
    "           loss = \"binary_crossentropy\",\n",
    "           metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model and save its history\n",
    "history = model.fit(X_train, y_train, epochs = 50, validation_data=(X_test,y_test))\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(history.history[\"acc\"],history.history[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback to monitor val_acc\n",
    "monitor_val_acc = EarlyStopping(monitor=\"val_acc\", patience=5)\n",
    "\n",
    "# Train your model using the early stopping callback\n",
    "model.fit(X_train,y_train, epochs=1000, validation_data=(X_test, y_test),\n",
    "           callbacks=[monitor_val_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the EarlyStopping and ModelCheckpoint callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "# Early stop on validation accuracy\n",
    "monitor_val_acc = EarlyStopping(monitor = \"val_acc\",patience=3)\n",
    "# Save the best model as best_banknote_model.hdf5\n",
    "modelCheckpoint = ModelCheckpoint(\"best_banknote_model.hdf5\", save_best_only = True)\n",
    "# Fit your model for a stupid amount of epochs\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs = 10000000,\n",
    "                    callbacks = [monitor_val_acc,modelCheckpoint],\n",
    "                    validation_data = (X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss,val_loss):\n",
    "  plt.figure()\n",
    "  plt.plot(loss)\n",
    "  plt.plot(val_loss)\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Test'], loc='upper right')\n",
    "  plt.show()\n",
    "def plot_loss(loss,val_loss):\n",
    "  plt.figure()\n",
    "  plt.plot(loss)\n",
    "  plt.plot(val_loss)\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Test'], loc='upper right')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input and hidden layer with input_shape, 16 neurons, and relu \n",
    "model.add(Dense(16, input_shape = (64,), activation = \"relu\"))\n",
    "\n",
    "# Output layer with 10 neurons (one per digit) and softmax\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "# Test if your model works and can process input data\n",
    "print(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 View loss after epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
    "history = model.fit(X_train, y_train, epochs=60, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# Extract from the history object loss and val_loss to plot the learning curve\n",
    "plot_loss(history.history[\"loss\"], history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/model_loss.png\" style=\"height:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. # of training examples w.r.t accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(train_accs,test_accs):\n",
    "  plt.plot(training_sizes, train_accs, 'o-', label=\"Training Accuracy\")\n",
    "  plt.plot(training_sizes, test_accs, 'o-', label=\"Test Accuracy\")\n",
    "  plt.title('Accuracy vs Number of training samples')\n",
    "  plt.xlabel('# of training samples')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend(loc=\"best\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = model.get_weights() # the models above\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "training_sizes = [125, 502, 879]\n",
    "for size in training_sizes:\n",
    "    # Get a fraction of training data (we only care about the training data)\n",
    "    X_train_frac, y_train_frac = X_train[:size], y_train[:size]\n",
    "    # Reset the model to the initial weights and train it on the new data fraction\n",
    "    model.set_weights(initial_weights)\n",
    "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [monitor_val_acc])\n",
    "\n",
    "    # Evaluate and store the train fraction and the complete test set results\n",
    "    train_accs.append(model.evaluate(X_train, y_train)[1])\n",
    "    test_accs.append(model.evaluate(X_test, y_test)[1])\n",
    "    \n",
    "# Plot train vs test accuracies\n",
    "plot_results(train_accs, test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/num_training.png\" style=\"height:400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 How models changes if we change activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The function get_model() returns a copy of model (we already build) and applies the activation function, passed on as a parameter, to its hidden layer --> not really understand how to use this using ModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def ReLU(x):\n",
    "    return np.maximum(0.0, x)\n",
    "def leaky_ReLU(x,alpha=0.01):\n",
    "    return np.maximum(alpha*x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(act_function):\n",
    "  if act_function not in ['relu', 'leaky_relu', 'sigmoid', 'tanh']:\n",
    "    raise ValueError('Make sure your activation functions are named correctly!')\n",
    "  print(\"Finishing with\",act_function,\"...\")\n",
    "  return ModelWrapper(act_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions to try\n",
    "activations = [\"relu\", \"leaky_relu\",\"sigmoid\", \"tanh\"]\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "val_loss_per_function ={}\n",
    "val_acc_per_function ={}\n",
    "\n",
    "for act in activations:\n",
    "  # Get a new model with the current activation\n",
    "  model = get_model(act_function=act)\n",
    "  # Fit the model\n",
    "  history = model.fit(X_train, y_train, validation_data =(X_test, y_test),epochs = 100, verbose=0)\n",
    "  activation_results[act] = history\n",
    "  val_loss_per_function[act] = history.history[\"val_loss\"]\n",
    "  val_acc_per_function[act] = history.history[\"val_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from val_loss_per_function\n",
    "val_loss= pd.DataFrame(val_loss_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_loss.plot()\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe from val_acc_per_function\n",
    "val_acc = pd.DataFrame(val_acc_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_acc.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/var_loss_with_different_activations.png\" style=\"height:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/var_acc_with_different_activations.png\" style=\"height:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 BatchNormalization or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build a models with batch normalization, a model without, then run and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import batch normalization from keras layers\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_histories_acc(h1,h2):\n",
    "  plt.plot(h1.history['acc'])\n",
    "  plt.plot(h1.history['val_acc'])\n",
    "  plt.plot(h2.history['acc'])\n",
    "  plt.plot(h2.history['val_acc'])\n",
    "  plt.title(\"Batch Normalization Effects\")\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend(['Train', 'Test', 'Train with Batch Normalization', 'Test with Batch Normalization'], loc='best')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model with batch normalization\n",
    "batchnorm_model = Sequential()\n",
    "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    "# Compile your model with sgd\n",
    "batchnorm_model.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a model without batch normalizaton\n",
    "standard_model = Sequential()\n",
    "standard_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
    "standard_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "standard_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "standard_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    "standard_model.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your standard model, storing its history\n",
    "history1 = standard_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Train the batch normalized model you recently built, store its history\n",
    "history2 = batchnorm_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Call compare_acc_histories passing in both model histories\n",
    "compare_histories_acc(history1, history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/batch_normalization.png\" style=\"height:400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Tuning with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model given an activation and learning rate\n",
    "def create_model(learning_rate=0.01, activation='relu'):\n",
    "  \n",
    "  # Create an Adam optimizer with the given learning rate\n",
    "  opt = Adam(lr=learning_rate)\n",
    "  \n",
    "  # Create your binary classification model  \n",
    "  model = Sequential()\n",
    "  model.add(Dense(128, input_shape=(30,), activation=activation))\n",
    "  model.add(Dense(256, activation=activation))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  \n",
    "  # Compile your model with your optimizer, loss, and metrics\n",
    "  model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this version if we want to tune number of hidden layers(nl), and number of units in each layer(nn)\n",
    "def create_model2(learning_rate=0.01, activation='relu', nl=1, nn=256):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(128, input_shape=(30,), activation=activation))\n",
    "  for i in range(nl):\n",
    "        model.add(Dense(nn, activation=activation))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  \n",
    "  # Compile your model with your optimizer, loss, and metrics\n",
    "  model.compile(optimizer=Adam(lr=learning_rate), loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KerasClassifier from keras wrappers. Using this wrapper, keras model \n",
    "# now can be used as an sklearn estimator, eg. we do cross_val_score\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "# Define the parameters to try out. We can also tune number of layers and number of nodes\n",
    "# in hidden layers. \n",
    "params = {'activation': [\"relu\", \"tanh\"], \n",
    "          'batch_size': [32, 128, 256], \n",
    "          'epochs': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "\n",
    "# Create a randomize search cv object passing in the parameters to try\n",
    "random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
    "random_search_results = random_search.fit(X, y)\n",
    "print(random_search_results.best_score_, random_search_results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After running the results select the best paramets: Best: 0.975395 using {learning_rate: 0.001, epochs: 50, batch_size: 128, activation: relu} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KerasClassifier from keras wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model, epochs = 50, \n",
    "             batch_size = 128, verbose = 0)\n",
    "\n",
    "# Calculate the accuracy score for each fold\n",
    "kfolds = cross_val_score(model, X, y, cv = 3)\n",
    "\n",
    "# Print the mean accuracy\n",
    "print('The mean accuracy was:', kfolds.mean())\n",
    "\n",
    "# Print the accuracy standard deviation\n",
    "print('With a standard deviation of:', kfolds.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
